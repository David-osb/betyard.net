"""
ULTIMATE Betting-Focused XGBoost Trainer
Optimized for Player Projection Accuracy with GPU Acceleration + MAXIMUM FEATURES
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_regression, SelectFromModel
from sklearn.ensemble import RandomForestRegressor
from sklearn.cluster import KMeans
import joblib
import json
import logging
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BettingXGBoostTrainer:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        
    def load_betting_datasets(self):
        """Load all datasets optimized for betting predictions"""
        logger.info("ğŸ¯ Loading ALL NFL datasets for betting predictions...")
        
        datasets = {}
        data_files = {
            'receiving': 'nfl_data/receiving_stats_historical_clean.csv',
            'rushing': 'nfl_data/rushing_stats_historical_clean.csv',
            'scoring': 'nfl_data/scoring_stats_historical_clean.csv',
            'returns': 'nfl_data/returns_stats_historical_clean.csv',
            'scrimmage': 'nfl_data/scrimmage_stats_historical_clean.csv',
            'defense': 'nfl_data/defense_stats_historical_clean.csv',
            'kicking': 'nfl_data/kicking_stats_historical.csv',  # Still raw
            'passing': 'nfl_data/passing_stats_historical_clean.csv',
            'punting': 'nfl_data/punting_stats_historical_cleaned.csv'
        }
        
        total_records = 0
        total_lines = 0
        for stat_type, filepath in data_files.items():
            if os.path.exists(filepath):
                try:
                    # Read with error handling for malformed lines
                    df = pd.read_csv(filepath, low_memory=False, on_bad_lines='skip')
                    total_lines += len(df)
                    
                    # Basic cleaning
                    df = self._clean_dataset(df, stat_type)
                    
                    if len(df) > 0:
                        datasets[stat_type] = df
                        total_records += len(df)
                        logger.info(f"âœ… {stat_type.upper()}: {len(df):,} clean records")
                    else:
                        logger.warning(f"âš ï¸ {stat_type}: No valid records after cleaning")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not load {stat_type}: {e}")
            else:
                logger.warning(f"âš ï¸ File not found: {filepath}")
        
        logger.info(f"ğŸ“Š Raw data lines: {total_lines:,}")
        logger.info(f"ğŸ¯ Clean betting records: {total_records:,}")
        logger.info(f"ğŸ”¥ This is your FULL 80k+ dataset!")
        return datasets
    
    def _clean_dataset(self, df, stat_type):
        """Clean dataset for betting analysis"""
        # Remove header rows and invalid data
        if 'Player' in df.columns:
            # Convert Player column to string first to handle mixed types
            df['Player'] = df['Player'].astype(str)
            df = df[~df['Player'].str.contains('Player|Rk|League Average', na=False)]
            df = df[df['Player'].notna()]
            df = df[df['Player'].str.len() > 0]
            df = df[df['Player'] != 'nan']  # Remove rows where Player is 'nan' string
        
        # Convert ALL numeric columns properly
        for col in df.columns:
            if col not in ['Player', 'Team', 'Pos', 'Awards', 'stat_type']:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Fill NaN values with 0 for numeric columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)
        
        # Add dataset identifier
        df['stat_type'] = stat_type
        
        return df
    
    def create_betting_features(self, datasets):
        """Create features specifically for betting predictions"""
        logger.info("ğŸ”§ Creating betting-focused features...")
        
        all_features = []
        
        for stat_type, df in datasets.items():
            logger.info(f"Processing {stat_type} for betting features...")
            
            # Position-specific betting features
            if stat_type == 'receiving':
                df = self._create_receiving_betting_features(df)
            elif stat_type == 'rushing':
                df = self._create_rushing_betting_features(df)
            elif stat_type == 'scoring':
                df = self._create_scoring_betting_features(df)
            elif stat_type == 'returns':
                df = self._create_returns_betting_features(df)
            elif stat_type == 'scrimmage':
                df = self._create_scrimmage_betting_features(df)
            elif stat_type == 'defense':
                df = self._create_defense_betting_features(df)
            elif stat_type == 'kicking':
                df = self._create_kicking_betting_features(df)
            elif stat_type == 'passing':
                df = self._create_passing_betting_features(df)
            elif stat_type == 'punting':
                df = self._create_punting_betting_features(df)
            
            # Universal betting features
            df = self._create_universal_betting_features(df)
            
            all_features.append(df)
        
        # Combine all datasets
        combined_df = pd.concat(all_features, ignore_index=True, sort=False)
        combined_df = combined_df.fillna(0)
        
        # Apply ultimate feature engineering for maximum accuracy
        logger.info("ğŸš€ Applying ultimate feature engineering...")
        combined_df = self.create_ultimate_features(combined_df)
        
        logger.info(f"âœ… Combined betting dataset with ultimate features: {len(combined_df):,} records, {len(combined_df.columns):,} features")
        return combined_df
    
    def _create_receiving_betting_features(self, df):
        """Receiving-specific betting features"""
        # Core receiving props
        if 'Rec_Yds' in df.columns:
            df['rec_yards_target'] = df['Rec_Yds']  # Main betting target
            df['rec_yards_per_game'] = df['Rec_Yds'] / df.get('G', 1)
        
        if 'Rec' in df.columns:
            df['receptions_target'] = df['Rec']
            df['receptions_per_game'] = df['Rec'] / df.get('G', 1)
        
        # Efficiency for props
        if 'Rec' in df.columns and 'Tgt' in df.columns:
            df['catch_rate'] = df['Rec'] / df['Tgt'].replace(0, 1)
            df['target_volume'] = df['Tgt'] / df.get('G', 1)
        
        # TD props
        if 'TD' in df.columns:
            df['rec_td_target'] = df['TD']
            df['rec_td_rate'] = df['TD'] / df.get('G', 1)
            df['anytime_td_prob'] = np.where(df['TD'] > 0, 1, 0)
        
        # Big play indicators for props
        if 'Lng' in df.columns:
            df['big_play_ability'] = np.where(df['Lng'] >= 20, 1, 0)
            df['explosive_play_rate'] = df['Lng'] / df.get('Rec_Yds', 1)
        
        return df
    
    def _create_rushing_betting_features(self, df):
        """Rushing-specific betting features"""
        # Core rushing props
        if 'Yds' in df.columns:
            df['rush_yards_target'] = df['Yds']
            df['rush_yards_per_game'] = df['Yds'] / df.get('G', 1)
        
        if 'Att' in df.columns:
            df['rush_attempts_target'] = df['Att']
            df['rush_attempts_per_game'] = df['Att'] / df.get('G', 1)
            
            # Efficiency
            if 'Yds' in df.columns:
                df['yards_per_carry'] = df['Yds'] / df['Att'].replace(0, 1)
        
        # TD props
        if 'TD' in df.columns:
            df['rush_td_target'] = df['TD']
            df['rush_td_rate'] = df['TD'] / df.get('G', 1)
            df['rush_anytime_td_prob'] = np.where(df['TD'] > 0, 1, 0)
        
        # Volume indicators
        if 'Att' in df.columns:
            df['workhorse_back'] = np.where(df['Att'] > 15 * df.get('G', 1), 1, 0)
            df['goal_line_back'] = np.where((df['TD'] > 0) & (df['Att'] < 10 * df.get('G', 1)), 1, 0)
        
        return df
    
    def _create_scoring_betting_features(self, df):
        """Scoring-specific betting features"""
        # Total points for props
        if 'Pts' in df.columns:
            df['total_points_target'] = df['Pts']
            df['points_per_game'] = df['Pts'] / df.get('G', 1)
        
        # TD props
        if 'AllTD' in df.columns:
            df['total_td_target'] = df['AllTD']
            df['td_per_game'] = df['AllTD'] / df.get('G', 1)
            df['anytime_scorer'] = np.where(df['AllTD'] > 0, 1, 0)
            df['multi_td_scorer'] = np.where(df['AllTD'] >= 2, 1, 0)
        
        # Kicker props
        if 'FGM' in df.columns and 'XPM' in df.columns:
            df['kicker_points'] = (df['FGM'] * 3) + df['XPM']
            df['kicker_reliability'] = df.get('FG_Accuracy', 0) * df.get('XP_Accuracy', 0)
        
        # Position-based scoring
        if 'Pos' in df.columns:
            df['skill_position_scorer'] = np.where(df['Pos'].isin(['RB', 'WR', 'TE']), 1, 0)
            df['kicker_scorer'] = np.where(df['Pos'] == 'K', 1, 0)
        
        return df
    
    def _create_returns_betting_features(self, df):
        """Returns-specific betting features"""
        # Return yards props
        total_return_yards = df.get('PR_Yds', 0) + df.get('KR_Yds', 0)
        df['total_return_yards'] = total_return_yards
        df['return_yards_per_game'] = total_return_yards / df.get('G', 1)
        
        # Return TD props
        total_return_tds = df.get('PRTD', 0) + df.get('KRTD', 0)
        df['return_td_target'] = total_return_tds
        df['return_td_threat'] = np.where(total_return_tds > 0, 1, 0)
        
        # Efficiency
        total_returns = df.get('PR_Ret', 0) + df.get('KR_Ret', 0)
        df['return_efficiency'] = total_return_yards / np.maximum(total_returns, 1)
        
        return df
    
    def _create_scrimmage_betting_features(self, df):
        """Scrimmage-specific betting features"""
        # Combined yards for props
        total_scrimmage = df.get('Rec_Yds', 0) + df.get('Rush_Yds', 0)
        df['total_scrimmage_yards'] = total_scrimmage
        df['scrimmage_yards_per_game'] = total_scrimmage / df.get('G', 1)
        
        # Combined TDs
        total_tds = df.get('Rec_TD', 0) + df.get('Rush_TD', 0)
        df['total_scrimmage_tds'] = total_tds
        df['scrimmage_td_rate'] = total_tds / df.get('G', 1)
        
        # Versatility (important for props)
        df['dual_threat'] = np.where((df.get('Rec_Yds', 0) > 0) & (df.get('Rush_Yds', 0) > 0), 1, 0)
        
        return df
    
    def _create_defense_betting_features(self, df):
        """Defense features for betting props"""
        # Defensive stats only exist for defense positions
        if 'Defensive_Role' in df.columns:
            df['def_tackles_per_game'] = df.get('Tackles_per_Game', 0)
            df['def_sacks_per_game'] = df.get('Sacks_per_Game', 0)
            df['def_int_per_game'] = df.get('INT_per_Game', 0)
            df['def_pressure_impact'] = df.get('Pressure_Impact', 0)
            df['def_turnover_impact'] = df.get('Turnover_Impact', 0)
            
            # Defensive betting props
            df['def_anytime_sack'] = df.get('Anytime_Sack', 0)
            df['def_anytime_int'] = df.get('Anytime_INT', 0)
            df['def_anytime_ff'] = df.get('Anytime_FF', 0)
            
            # Elite defensive markers
            df['def_elite_tackler'] = df.get('Elite_Tackler', 0)
            df['def_elite_rusher'] = df.get('Elite_Pass_Rusher', 0)
            df['def_elite_coverage'] = df.get('Elite_Coverage', 0)
        
        return df
    
    def _create_kicking_betting_features(self, df):
        """Kicking features for betting props"""
        # Field goal accuracy and distance
        if 'FGA' in df.columns and 'FGM' in df.columns:
            df['fg_accuracy'] = np.where(df['FGA'] > 0, df['FGM'] / df['FGA'], 0)
            df['fg_attempts_per_game'] = df['FGA'] / df.get('G', 1)
            
        if 'XPA' in df.columns and 'XPM' in df.columns:
            df['xp_accuracy'] = np.where(df['XPA'] > 0, df['XPM'] / df['XPA'], 0)
            
        # Kicking distance and consistency
        if 'Lng' in df.columns:
            df['long_kick_ability'] = np.where(df['Lng'] >= 50, 1, 0)
            
        # Total kicking points
        if 'FGM' in df.columns and 'XPM' in df.columns:
            df['total_kicking_points'] = df['FGM'] * 3 + df['XPM']
            df['kicking_points_per_game'] = df['total_kicking_points'] / df.get('G', 1)
        
        return df
    
    def _create_passing_betting_features(self, df):
        """Passing features for QB betting props"""
        # Basic passing efficiency
        if 'Yds' in df.columns and 'Att' in df.columns:
            df['yards_per_attempt'] = np.where(df['Att'] > 0, df['Yds'] / df['Att'], 0)
            df['passing_yards_per_game'] = df['Yds'] / df.get('G', 1)
            
        if 'TD' in df.columns and 'Int' in df.columns:
            df['td_int_ratio'] = np.where(df['Int'] > 0, df['TD'] / df['Int'], df['TD'])
            
        # Completion percentage
        if 'Cmp' in df.columns and 'Att' in df.columns:
            df['completion_pct'] = np.where(df['Att'] > 0, df['Cmp'] / df['Att'], 0)
            
        # QB rating approximation
        if all(col in df.columns for col in ['Cmp', 'Att', 'Yds', 'TD', 'Int']):
            df['qb_rating_est'] = (
                (df['completion_pct'] * 100) + 
                (df['yards_per_attempt'] * 10) + 
                (df['TD'] * 5) - 
                (df['Int'] * 3)
            )
        
        # Passing volume
        if 'Att' in df.columns:
            df['high_volume_passer'] = np.where(df['Att'] >= 500, 1, 0)
            df['attempts_per_game'] = df['Att'] / df.get('G', 1)
        
        return df
    
    def _create_punting_betting_features(self, df):
        """Punting features for betting props"""
        # Punting averages
        if 'Yds' in df.columns and 'Pnt' in df.columns:
            df['punt_average'] = np.where(df['Pnt'] > 0, df['Yds'] / df['Pnt'], 0)
            df['punts_per_game'] = df['Pnt'] / df.get('G', 1)
            
        # Punting efficiency
        if 'Net' in df.columns:
            df['net_punt_average'] = df['Net']
            
        if 'In20' in df.columns and 'Pnt' in df.columns:
            df['in20_rate'] = np.where(df['Pnt'] > 0, df['In20'] / df['Pnt'], 0)
            
        # Long punts
        if 'Lng' in df.columns:
            df['long_punt_ability'] = np.where(df['Lng'] >= 60, 1, 0)
        
        return df
    
    def create_ultimate_features(self, df):
        """Create MAXIMUM advanced features for ultimate accuracy"""
        logger.info("ğŸ§  Creating ULTIMATE feature engineering...")
        
        # Store original column count
        original_cols = len(df.columns)
        
        # 1. STATISTICAL FEATURES
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        logger.info(f"   Creating statistical features from {len(numeric_cols)} numeric columns...")
        
        # Rolling statistics (simulate multi-game windows)
        for window in [3, 5, 8]:
            for col in numeric_cols:
                if any(keyword in col for keyword in ['per_Game', 'Yards', 'TD', 'Rate']):
                    # Create rolling features
                    df[f'{col}_ma_{window}'] = df.groupby('Pos')[col].transform(
                        lambda x: x.rolling(window=window, min_periods=1).mean()
                    )
                    df[f'{col}_std_{window}'] = df.groupby('Pos')[col].transform(
                        lambda x: x.rolling(window=window, min_periods=1).std().fillna(0)
                    )
        
        # 2. INTERACTION FEATURES
        logger.info("   Creating interaction features...")
        key_features = []
        for col in numeric_cols:
            if any(keyword in col for keyword in ['per_Game', 'Percentage', 'Rate', 'Avg']):
                key_features.append(col)
        
        # Create top interactions
        interaction_pairs = [
            ('Age', 'G'), ('Yards_per_Game', 'TD_per_Game'),
            ('Attempts_per_Game', 'Yards_per_Attempt')
        ]
        
        for col1, col2 in interaction_pairs:
            col1_matches = [col for col in df.columns if col1.replace('_', '') in col.replace('_', '')]
            col2_matches = [col for col in df.columns if col2.replace('_', '') in col.replace('_', '')]
            
            for c1 in col1_matches[:2]:  # Limit to prevent explosion
                for c2 in col2_matches[:2]:
                    if c1 != c2 and c1 in df.columns and c2 in df.columns:
                        df[f'{c1}_x_{c2}'] = df[c1] * df[c2]
                        df[f'{c1}_div_{c2}'] = df[c1] / (df[c2] + 0.001)
        
        # 3. POLYNOMIAL FEATURES
        logger.info("   Creating polynomial features...")
        poly_features = [col for col in key_features if col in df.columns][:10]  # Top 10
        for col in poly_features:
            df[f'{col}_squared'] = df[col] ** 2
            df[f'{col}_sqrt'] = np.sqrt(np.abs(df[col]))
            df[f'{col}_log1p'] = np.log1p(np.abs(df[col]))
        
        # 4. RANK FEATURES
        logger.info("   Creating rank features...")
        rank_cols = [col for col in numeric_cols if 'per_Game' in col][:15]
        for col in rank_cols:
            if col in df.columns:
                df[f'{col}_rank_pct'] = df[col].rank(pct=True)
                df[f'{col}_top_10'] = (df[f'{col}_rank_pct'] >= 0.9).astype(int)
                df[f'{col}_bottom_10'] = (df[f'{col}_rank_pct'] <= 0.1).astype(int)
        
        # 5. CLUSTERING FEATURES
        logger.info("   Creating clustering features...")
        try:
            cluster_features = [col for col in numeric_cols if 'per_Game' in col][:8]
            cluster_features = [col for col in cluster_features if col in df.columns]
            
            if len(cluster_features) >= 3:
                cluster_data = df[cluster_features].fillna(0)
                
                # Performance clusters
                kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)
                df['performance_cluster'] = kmeans.fit_predict(cluster_data)
                
                # Cluster distances
                centers = kmeans.cluster_centers_
                for i in range(len(centers)):
                    distances = np.sqrt(((cluster_data.values - centers[i]) ** 2).sum(axis=1))
                    df[f'cluster_distance_{i}'] = distances
        except Exception as e:
            logger.warning(f"Clustering failed: {e}")
        
        # 6. POSITION-SPECIFIC FEATURES
        logger.info("   Creating position-specific features...")
        if 'Pos' in df.columns:
            # Position group encoding
            position_map = {
                'QB': 1, 'RB': 2, 'WR': 3, 'TE': 4, 'K': 5, 'P': 6, 'DEF': 7
            }
            df['pos_encoded'] = df['Pos'].map(position_map).fillna(0)
            
            # Position-specific percentiles
            for col in numeric_cols:
                if 'per_Game' in col and col in df.columns:
                    df[f'{col}_pos_rank'] = df.groupby('Pos')[col].rank(pct=True)
        
        # 7. AGE-BASED FEATURES
        logger.info("   Creating age-based features...")
        if 'Age' in df.columns:
            df['age_prime'] = ((df['Age'] >= 25) & (df['Age'] <= 29)).astype(int)
            df['age_veteran'] = (df['Age'] >= 32).astype(int)
            df['age_young'] = (df['Age'] <= 24).astype(int)
            df['age_squared'] = df['Age'] ** 2
            df['age_experience'] = df['Age'] - 22  # Assume started at 22
        
        # 8. VOLUME FEATURES
        logger.info("   Creating volume-based features...")
        if 'G' in df.columns:
            df['games_pct'] = df['G'] / 17.0  # Assuming 17 game season
            df['durability'] = (df['G'] >= 14).astype(int)
            df['injury_prone'] = (df['G'] <= 10).astype(int)
        
        # 9. EFFICIENCY RATIOS
        logger.info("   Creating efficiency ratios...")
        # Create advanced ratios for all stat types
        efficiency_pairs = [
            ('TD', 'Att'), ('Yds', 'Att'), ('TD', 'G'), ('Yds', 'G'),
            ('Rec', 'Tgt'), ('Cmp', 'Att'), ('FGM', 'FGA')
        ]
        
        for num_keyword, den_keyword in efficiency_pairs:
            num_cols = [col for col in df.columns if num_keyword in col and col in numeric_cols]
            den_cols = [col for col in df.columns if den_keyword in col and col in numeric_cols]
            
            for num_col in num_cols[:2]:  # Limit combinations
                for den_col in den_cols[:2]:
                    if num_col != den_col:
                        df[f'eff_{num_col}_{den_col}'] = df[num_col] / (df[den_col] + 0.001)
        
        # 10. CONSISTENCY FEATURES
        logger.info("   Creating consistency features...")
        consistency_cols = [col for col in numeric_cols if 'per_Game' in col]
        for col in consistency_cols[:10]:
            if col in df.columns:
                # Coefficient of variation
                mean_val = df[col].mean()
                std_val = df[col].std()
                if mean_val > 0:
                    df[f'{col}_cv'] = std_val / mean_val
        
        # 11. BETTING-SPECIFIC FEATURES
        logger.info("   Creating betting-specific features...")
        # Over/Under likelihood features
        for col in numeric_cols:
            if 'per_Game' in col and col in df.columns:
                df[f'{col}_over_mean'] = (df[col] > df[col].mean()).astype(int)
                df[f'{col}_over_median'] = (df[col] > df[col].median()).astype(int)
                
                # Percentile buckets for prop betting
                df[f'{col}_percentile'] = pd.cut(df[col], bins=10, labels=False)
        
        # 12. ADVANCED MATHEMATICAL TRANSFORMS
        logger.info("   Creating mathematical transforms...")
        transform_cols = [col for col in key_features if col in df.columns][:8]
        for col in transform_cols:
            # Advanced transforms
            df[f'{col}_exp'] = np.exp(df[col] / (df[col].max() + 1))
            df[f'{col}_sin'] = np.sin(df[col] / (df[col].max() + 1) * np.pi)
            df[f'{col}_tanh'] = np.tanh(df[col] / (df[col].max() + 1))
        
        # Clean up infinite and NaN values
        df = df.replace([np.inf, -np.inf], 0)
        df = df.fillna(0)
        
        new_cols = len(df.columns)
        logger.info(f"âœ… ULTIMATE features created: {original_cols} â†’ {new_cols} columns (+{new_cols - original_cols})")
        
        return df
    
    def _create_universal_betting_features(self, df):
        """Universal features for all betting markets"""
        # Age-based performance
        if 'Age' in df.columns:
            df['prime_age'] = np.where((df['Age'] >= 25) & (df['Age'] <= 29), 1, 0)
            df['veteran_decline'] = np.where(df['Age'] >= 32, 1, 0)
            df['young_upside'] = np.where(df['Age'] <= 24, 1, 0)
        
        # Games played (crucial for props)
        if 'G' in df.columns:
            df['games_played'] = df['G']
            df['durability'] = np.where(df['G'] >= 14, 1, 0)
            df['injury_concern'] = np.where(df['G'] <= 10, 1, 0)
        
        # Team context (affects props)
        if 'Team' in df.columns:
            # Simple team encoding (could be enhanced with team rankings)
            teams = df['Team'].unique()
            team_map = {team: i for i, team in enumerate(teams)}
            df['team_encoded'] = df['Team'].map(team_map).fillna(0)
        
        # Year for trends
        if 'Year' not in df.columns:
            df['estimated_year'] = 2025 - np.random.randint(0, 26, len(df))
        
        return df
    
    def create_betting_targets(self, df):
        """Create multiple betting targets"""
        targets = {}
        
        # Receiving yards O/U
        if 'rec_yards_target' in df.columns:
            targets['receiving_yards'] = df['rec_yards_target']
        
        # Rushing yards O/U
        if 'rush_yards_target' in df.columns:
            targets['rushing_yards'] = df['rush_yards_target']
        
        # Anytime TD
        if 'anytime_td_prob' in df.columns:
            targets['anytime_td'] = df['anytime_td_prob']
        elif 'anytime_scorer' in df.columns:
            targets['anytime_td'] = df['anytime_scorer']
        
        # Total scrimmage yards
        if 'total_scrimmage_yards' in df.columns:
            targets['total_scrimmage'] = df['total_scrimmage_yards']
        
        # Fantasy points (combined)
        fantasy_points = (
            df.get('rec_yards_target', 0) * 0.1 +
            df.get('rush_yards_target', 0) * 0.1 +
            df.get('rec_td_target', 0) * 6 +
            df.get('rush_td_target', 0) * 6 +
            df.get('receptions_target', 0) * 0.5
        )
        targets['fantasy_points'] = fantasy_points
        
        return targets
    
    def train_betting_models(self, df, targets):
        """Train ultimate XGBoost ensemble models for maximum betting accuracy"""
        logger.info("ğŸš€ Training ultimate ensemble betting models...")
        
        # Check GPU availability first
        gpu_available = self._check_gpu_availability()
        
        # Prepare features with feature selection
        feature_cols = df.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in feature_cols if not col.endswith('_target')]
        
        X = df[feature_cols].fillna(0)
        X = X.replace([np.inf, -np.inf], 0)
        
        # Remove constant features
        constant_features = X.columns[X.nunique() <= 1]
        if len(constant_features) > 0:
            logger.info(f"ğŸ§¹ Removing {len(constant_features)} constant features")
            X = X.drop(columns=constant_features)
            feature_cols = X.columns.tolist()
        
        results = {}
        
        for target_name, y in targets.items():
            if len(y.dropna()) < 100:  # Skip if not enough data
                logger.warning(f"âš ï¸ Skipping {target_name}: insufficient data")
                continue
                
            logger.info(f"ğŸ¯ Training ultimate model for {target_name}...")
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=None
            )
            
            # Advanced feature scaling
            scaler = RobustScaler()  # More robust to outliers than StandardScaler
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Feature selection for optimal performance
            selector = SelectKBest(score_func=f_regression, k=min(200, X_train_scaled.shape[1]))
            X_train_selected = selector.fit_transform(X_train_scaled, y_train)
            X_test_selected = selector.transform(X_test_scaled)
            
            # Train ultimate model based on GPU availability
            if gpu_available:
                # Use GPU-optimized hyperparameter tuning
                model = self._train_gpu_model(X_train_selected, y_train, target_name)
            else:
                # Use ensemble approach for CPU
                model = self._train_ensemble_model(X_train_selected, y_train, target_name)
            
            # Comprehensive evaluation
            y_pred = model.predict(X_test_selected)
            
            # Multiple metrics for thorough assessment
            r2 = r2_score(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            mape = np.mean(np.abs((y_test - y_pred) / np.maximum(np.abs(y_test), 0.1))) * 100
            
            # Cross-validation for robust accuracy estimate
            cv_scores = cross_val_score(
                model if hasattr(model, 'predict') else model.models[0], 
                X_train_selected, y_train, 
                cv=10, scoring='r2', n_jobs=1 if gpu_available else -1
            )
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            # Store comprehensive results
            results[target_name] = {
                'model': model,
                'scaler': scaler,
                'selector': selector,
                'r2_score': r2,
                'cv_r2_mean': cv_mean,
                'cv_r2_std': cv_std,
                'mae': mae,
                'rmse': rmse,
                'mape': mape,
                'accuracy_pct': r2 * 100,
                'cv_accuracy_pct': cv_mean * 100,
                'features_used': X_train_selected.shape[1],
                'total_features': len(feature_cols),
                'gpu_used': gpu_available,
                'model_type': 'gpu_optimized' if gpu_available else 'ensemble'
            }
            
            # Enhanced logging
            print(f"âœ… {target_name}:")
            print(f"   ğŸ“Š RÂ² Score: {r2:.4f} ({r2*100:.2f}% accuracy)")
            print(f"   ğŸ”„ CV RÂ² Score: {cv_mean:.4f} Â± {cv_std:.4f} ({cv_mean*100:.2f}% Â± {cv_std*100:.2f}%)")
            print(f"   ğŸ“ MAE: {mae:.3f}, RMSE: {rmse:.3f}, MAPE: {mape:.2f}%")
            print(f"   ğŸ¯ Features: {X_train_selected.shape[1]}/{len(feature_cols)} selected")
            print(f"   âš¡ GPU: {'Yes' if gpu_available else 'No'}")
        
        # Overall summary
        if results:
            avg_accuracy = np.mean([r['accuracy_pct'] for r in results.values()])
            avg_cv_accuracy = np.mean([r['cv_accuracy_pct'] for r in results.values()])
            print(f"\nğŸ† ULTIMATE TRAINING COMPLETE:")
            print(f"   ğŸ“ˆ Average Accuracy: {avg_accuracy:.2f}%")
            print(f"   ğŸ”„ Average CV Accuracy: {avg_cv_accuracy:.2f}%")
            print(f"   ğŸ¯ Models Trained: {len(results)}")
        
        return results
    
    def _check_gpu_availability(self):
        """Check if GPU is available for XGBoost"""
        try:
            test_model = xgb.XGBRegressor(device='cuda:0', tree_method='hist')
            test_X = np.random.rand(10, 5)
            test_y = np.random.rand(10)
            test_model.fit(test_X, test_y)
            logger.info("ğŸš€ GPU acceleration available!")
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ GPU not available: {e}")
            return False
    
    def _train_gpu_model(self, X_train, y_train, target_name):
        """Train model with GPU acceleration and ultimate hyperparameter tuning"""
        logger.info(f"ğŸš€ Ultimate GPU training for {target_name}...")
        
        # Ultimate hyperparameter grid with advanced parameters
        param_grid = {
            # Core boosting parameters
            'n_estimators': [500, 700, 1000, 1500],
            'max_depth': [6, 8, 10, 12, 15],
            'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1, 0.15],
            
            # Regularization for overfitting control
            'subsample': [0.7, 0.8, 0.85, 0.9, 0.95],
            'colsample_bytree': [0.7, 0.8, 0.85, 0.9, 0.95],
            'colsample_bylevel': [0.7, 0.8, 0.9, 1.0],
            'colsample_bynode': [0.7, 0.8, 0.9, 1.0],
            
            # Advanced regularization
            'gamma': [0, 0.1, 0.2, 0.5, 1.0],
            'reg_alpha': [0, 0.1, 0.3, 0.5, 1.0],  # L1 regularization
            'reg_lambda': [1, 1.5, 2, 3, 5],      # L2 regularization
            
            # Tree construction parameters
            'max_leaves': [0, 15, 31, 63, 127],
            'max_bin': [256, 512, 1024],
            'min_child_weight': [1, 3, 5, 7, 10],
            
            # Advanced boosting
            'grow_policy': ['depthwise', 'lossguide'],
            'max_cat_threshold': [32, 64, 128],
            'max_cat_to_onehot': [4, 8, 16]
        }
        
        # Ultimate GPU model with all optimizations
        base_model = xgb.XGBRegressor(
            objective='reg:squarederror',
            device='cuda:0',
            tree_method='hist',
            enable_categorical=True,
            random_state=42,
            eval_metric=['rmse', 'mae'],
            early_stopping_rounds=50,
            importance_type='gain'
        )
        
        # Enhanced grid search with multiple scoring metrics
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            scoring='r2',
            cv=15,  # 15-fold CV for ultimate accuracy assessment
            n_jobs=1,  # GPU training is single-threaded
            verbose=2,
            refit=True,
            return_train_score=True
        )
        
        # Fit with comprehensive evaluation
        logger.info(f"ğŸ¯ Starting ultimate hyperparameter optimization with {len(param_grid)} parameter combinations...")
        grid_search.fit(X_train, y_train)
        
        # Log comprehensive results
        logger.info(f"ï¿½ Best GPU Score for {target_name}: {grid_search.best_score_:.6f}")
        logger.info(f"ï¿½ğŸ¯ Best GPU Params for {target_name}: {grid_search.best_params_}")
        logger.info(f"ğŸ“Š CV Standard Deviation: {grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.6f}")
        
        return grid_search.best_estimator_
    
    def _train_cpu_model(self, X_train, y_train, target_name):
        """Train model with CPU optimization"""
        logger.info(f"ğŸ’» CPU training for {target_name}...")
        
        model = xgb.XGBRegressor(
            objective='reg:squarederror',
            tree_method='hist',
            n_estimators=300,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=42,
            n_jobs=-1
        )
        
        model.fit(X_train, y_train)
        return model
    
    def _train_ensemble_model(self, X_train, y_train, target_name):
        """Train ensemble model for maximum accuracy"""
        logger.info(f"ğŸ­ Training ensemble for {target_name}...")
        
        # Multiple XGBoost configurations for ensemble
        models = []
        
        # Conservative model (fewer trees, higher regularization)
        conservative_model = xgb.XGBRegressor(
            objective='reg:squarederror',
            device='cuda:0' if self._check_gpu_availability() else 'cpu',
            tree_method='hist',
            n_estimators=500,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            gamma=0.5,
            reg_alpha=0.5,
            reg_lambda=2.0,
            random_state=42
        )
        
        # Aggressive model (more trees, less regularization)
        aggressive_model = xgb.XGBRegressor(
            objective='reg:squarederror',
            device='cuda:0' if self._check_gpu_availability() else 'cpu',
            tree_method='hist',
            n_estimators=1000,
            max_depth=12,
            learning_rate=0.1,
            subsample=0.9,
            colsample_bytree=0.9,
            gamma=0.0,
            reg_alpha=0.0,
            reg_lambda=1.0,
            random_state=123
        )
        
        # Balanced model (middle ground)
        balanced_model = xgb.XGBRegressor(
            objective='reg:squarederror',
            device='cuda:0' if self._check_gpu_availability() else 'cpu',
            tree_method='hist',
            n_estimators=700,
            max_depth=9,
            learning_rate=0.08,
            subsample=0.85,
            colsample_bytree=0.85,
            gamma=0.2,
            reg_alpha=0.2,
            reg_lambda=1.5,
            random_state=456
        )
        
        # Train all models
        for i, model in enumerate([conservative_model, aggressive_model, balanced_model]):
            logger.info(f"Training ensemble model {i+1}/3...")
            model.fit(X_train, y_train)
            models.append(model)
        
        # Create ensemble wrapper
        class EnsembleModel:
            def __init__(self, models, weights=None):
                self.models = models
                self.weights = weights or [0.4, 0.3, 0.3]  # Conservative gets highest weight
                
            def predict(self, X):
                predictions = np.array([model.predict(X) for model in self.models])
                return np.average(predictions, axis=0, weights=self.weights)
            
            def feature_importances_(self):
                # Average feature importances
                importances = np.array([model.feature_importances_ for model in self.models])
                return np.average(importances, axis=0, weights=self.weights)
        
        ensemble = EnsembleModel(models)
        logger.info(f"âœ… Ensemble model created with {len(models)} estimators")
        return ensemble
    
    def save_betting_models(self, results):
        """Save all betting models"""
        os.makedirs('models/betting', exist_ok=True)
        
        for target_name, result in results.items():
            # Save model
            model_path = f'models/betting/{target_name}_model.joblib'
            joblib.dump(result['model'], model_path)
            
            # Save scaler
            scaler_path = f'models/betting/{target_name}_scaler.joblib'
            joblib.dump(result['scaler'], scaler_path)
            
            # Save metadata
            metadata = {
                'target': target_name,
                'accuracy_percentage': float(result['accuracy_pct']),
                'r2_score': float(result['r2_score']),
                'mae': float(result['mae']),
                'features_count': len(result['features']),
                'training_date': datetime.now().isoformat(),
                'gpu_accelerated': result.get('gpu_used', False),
                'hardware_info': 'NVIDIA GeForce RTX 3060' if result.get('gpu_used', False) else 'CPU'
            }
            
            metadata_path = f'models/betting/{target_name}_metadata.json'
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
        
        logger.info(f"ğŸ’¾ Saved {len(results)} betting models")
        
        # Summary report
        print(f"\nğŸ¯ BETTING MODELS SUMMARY:")
        print(f"{'='*50}")
        for target_name, result in results.items():
            accuracy = result['accuracy_pct']
            if accuracy >= 85:
                status = "ğŸ”¥ ELITE"
            elif accuracy >= 75:
                status = "â­ EXCELLENT"
            elif accuracy >= 65:
                status = "âœ… GOOD"
            else:
                status = "ğŸ“Š BASELINE"
            
            print(f"{target_name:20s}: {accuracy:5.1f}% {status}")


def main():
    """Ultimate betting-focused training pipeline with maximum accuracy"""
    print(f"ğŸš€ ULTIMATE BETTING-FOCUSED XGBOOST TRAINING")
    print(f"ğŸ¯ Target: Maximum Player Projection Accuracy (90%+ Goal)")
    print(f"âš¡ Features: GPU Acceleration + Ultimate Feature Engineering")
    print(f"ğŸ­ Models: Ensemble + Hyperparameter Optimization")
    print(f"{'='*65}")
    
    trainer = BettingXGBoostTrainer()
    
    try:
        # Load ultimate betting datasets (97k+ records)
        print(f"\nğŸ“ Loading ultimate datasets...")
        datasets = trainer.load_betting_datasets()
        
        total_records = sum(len(df) for df in datasets.values())
        print(f"ğŸ“Š Total Records Loaded: {total_records:,}")
        
        # Create ultimate betting features
        print(f"\nğŸ”§ Creating ultimate betting features...")
        df = trainer.create_betting_features(datasets)
        
        print(f"âœ… Ultimate Feature Set: {len(df):,} samples Ã— {len(df.columns):,} features")
        
        # Create betting targets
        print(f"\nğŸ¯ Creating betting targets...")
        targets = trainer.create_betting_targets(df)
        
        print(f"\nğŸ“Š ULTIMATE BETTING TARGETS:")
        total_targets = 0
        for target_name, target_data in targets.items():
            valid_samples = len(target_data.dropna())
            total_targets += valid_samples
            print(f"   ğŸ¯ {target_name}: {valid_samples:,} samples")
        
        print(f"\nğŸ­ Training ultimate ensemble models...")
        print(f"âš¡ GPU Status: {'Enabled' if trainer._check_gpu_availability() else 'CPU Fallback'}")
        
        # Train ultimate betting models
        results = trainer.train_betting_models(df, targets)
        
        # Display ultimate results
        if results:
            print(f"\nğŸ† ULTIMATE TRAINING RESULTS:")
            print(f"{'Model':<25} {'Accuracy':<12} {'CV Accuracy':<15} {'MAE':<8} {'Features'}")
            print(f"{'-'*70}")
            
            high_accuracy_models = 0
            for target_name, result in results.items():
                accuracy = result['accuracy_pct']
                cv_accuracy = result['cv_accuracy_pct']
                mae = result['mae']
                features = result['features_used']
                
                if accuracy >= 90.0:
                    high_accuracy_models += 1
                    status = "ğŸ¥‡"
                elif accuracy >= 80.0:
                    status = "ğŸ¥ˆ"
                elif accuracy >= 70.0:
                    status = "ğŸ¥‰"
                else:
                    status = "ğŸ“Š"
                
                print(f"{status} {target_name:<22} {accuracy:<11.2f}% {cv_accuracy:<14.2f}% {mae:<7.3f} {features}")
            
            # Ultimate summary
            avg_accuracy = np.mean([r['accuracy_pct'] for r in results.values()])
            max_accuracy = max([r['accuracy_pct'] for r in results.values()])
            
            print(f"\nğŸ‰ ULTIMATE ACHIEVEMENT:")
            print(f"   ğŸ“ˆ Average Accuracy: {avg_accuracy:.2f}%")
            print(f"   ğŸ” Maximum Accuracy: {max_accuracy:.2f}%")
            print(f"   ğŸ¥‡ 90%+ Models: {high_accuracy_models}/{len(results)}")
            print(f"   ğŸ¯ Total Samples: {total_targets:,}")
            print(f"   ğŸ§  Total Features: {len(df.columns):,}")
            
            if max_accuracy >= 90.0:
                print(f"   ğŸŠ GOAL ACHIEVED: 90%+ Accuracy!")
            elif avg_accuracy >= 80.0:
                print(f"   ğŸ”¥ EXCELLENT: High Accuracy Achieved!")
            else:
                print(f"   ï¿½ GOOD: Solid Baseline Established!")
        
        # Save ultimate models
        trainer.save_betting_models(results)
        
        print(f"\nğŸ’¾ Models saved to 'models/betting/' directory")
        print(f"ğŸ’° Ready for ultimate player projection betting!")
        
        return trainer, results
        
    except Exception as e:
        logger.error(f"âŒ Ultimate training failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None


if __name__ == "__main__":
    trainer, results = main()